{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2083,
     "status": "ok",
     "timestamp": 1714576494369,
     "user": {
      "displayName": "Robin",
      "userId": "02323972527031131878"
     },
     "user_tz": -120
    },
    "id": "-wR6jF7NbPTW",
    "outputId": "72bbb5c5-d27d-4ef3-b192-95ac49b200e5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbW88w4rbUzX"
   },
   "source": [
    "# **Coursework 10 CNN-RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11906,
     "status": "ok",
     "timestamp": 1714576506273,
     "user": {
      "displayName": "Robin",
      "userId": "02323972527031131878"
     },
     "user_tz": -120
    },
    "id": "rZsGEl0qcNV8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D,TimeDistributed, Concatenate\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u0PTpMNbgtJ"
   },
   "source": [
    "### Part 1 :: import the dataset and convert video into numpy array\n",
    "\n",
    "\n",
    "*   3 dataset wave, boxing, clapping\n",
    "*   show 2 images for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1714576507547,
     "user": {
      "displayName": "Robin",
      "userId": "02323972527031131878"
     },
     "user_tz": -120
    },
    "id": "MFCSE-JObTrZ",
    "outputId": "f102101f-0907-4b51-992d-3929c10ab16b"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Video frame\n",
    "def video_to_frames(video_path, color_mode=cv2.COLOR_BGR2GRAY, max_frames=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or len(frames) == max_frames:\n",
    "                break\n",
    "            gray_frame = cv2.cvtColor(frame, color_mode)\n",
    "            frames.append(gray_frame)\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def plot_first_two_frames(frames):\n",
    "    # Check if there are at least two frames to plot\n",
    "    if frames.shape[0] < 2:\n",
    "        print(\"The array does not contain enough frames.\")\n",
    "        return\n",
    "    # Set up the figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # Plot the first frame\n",
    "    axes[0].imshow(frames[0], cmap='gray')\n",
    "    axes[0].set_title('First Frame')\n",
    "    axes[0].axis('off')  # Hide axes ticks\n",
    "    # Plot the second frame\n",
    "    axes[1].imshow(frames[1], cmap='gray')\n",
    "    axes[1].set_title('Second Frame')\n",
    "    axes[1].axis('off')  # Hide axes ticks\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video_path_box = '/content/drive/MyDrive/MICRO-573/cw10/data/boxing/person01_boxing_d1_uncomp.avi'\n",
    "video_path_clap = '/content/drive/MyDrive/MICRO-573/cw10/data/handclapping/person01_handclapping_d1_uncomp.avi'\n",
    "video_path_wave = '/content/drive/MyDrive/MICRO-573/cw10/data/handwaving/person01_handwaving_d1_uncomp.avi'\n",
    "\n",
    "frames_wave = video_to_frames(video_path_wave)\n",
    "frames_box = video_to_frames(video_path_box)\n",
    "frames_clap = video_to_frames(video_path_clap)\n",
    "\n",
    "\n",
    "print(frames_wave.shape)  # Output the shape to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 987
    },
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1714576509270,
     "user": {
      "displayName": "Robin",
      "userId": "02323972527031131878"
     },
     "user_tz": -120
    },
    "id": "5-FDjJ-igp2R",
    "outputId": "0dafd303-a095-487a-f0d7-5823e6bf089b"
   },
   "outputs": [],
   "source": [
    "# Showing the first images of each dataset\n",
    "print('Plotting the first two frame of class : waving')\n",
    "plot_first_two_frames(frames_wave)\n",
    "print('Plotting the first two frame of class : boxing')\n",
    "plot_first_two_frames(frames_box)\n",
    "print('Plotting the first two frame of class : claping')\n",
    "plot_first_two_frames(frames_clap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvkpjQvfeMQN"
   },
   "source": [
    "## Part 2 :: CNN-RNN Training and testing\n",
    "\n",
    "\n",
    "*   Implement the network depicted in Figure below with 32 neurons in the hidden layer that is before the output layer.\n",
    "* Use only two frames/sequences.\n",
    "*    Report your test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17460,
     "status": "ok",
     "timestamp": 1714576526727,
     "user": {
      "displayName": "Robin",
      "userId": "02323972527031131878"
     },
     "user_tz": -120
    },
    "id": "Sc3vkJIXefAs"
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset for the part 2\n",
    "\n",
    "\"\"\"\n",
    "This part will have focus on the creation of a train/test dataset as well as the labelling of the data.\n",
    "\"\"\"\n",
    "\n",
    "def load_data_from_folders(folder_list, labels,\n",
    "                           frames_per_video=2,\n",
    "                           max_frames=100):\n",
    "    \"\"\"\n",
    "    Loads data and labels from a list of folders.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    label_data = []\n",
    "\n",
    "    for folder, label in zip(folder_list, labels):\n",
    "        videos = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.avi')]\n",
    "        for video in videos:\n",
    "            video_frames = video_to_frames(video, max_frames=max_frames) # extract the first 10 frames\n",
    "            for i in range(0, max_frames, frames_per_video):\n",
    "                data.append(video_frames[i:i + frames_per_video])\n",
    "                label_data.append(label)\n",
    "\n",
    "    return np.array(data), np.array(label_data)\n",
    "\n",
    "# Example folder paths and labels\n",
    "folders = ['/content/drive/MyDrive/MICRO-573/cw10/data/handwaving',\n",
    "           '/content/drive/MyDrive/MICRO-573/cw10/data/handclapping',\n",
    "           '/content/drive/MyDrive/MICRO-573/cw10/data/boxing']\n",
    "labels = [0, # 0=Waving,\n",
    "          1, # 1=Clapping,\n",
    "          2] # 2=Boxing\n",
    "\n",
    "# Create the dataset\n",
    "data, labels = load_data_from_folders(folders, labels)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_one_hot = to_categorical(labels)\n",
    "# Add extra dimension for channel CNN\n",
    "data_with_channel = np.expand_dims(data, axis=-1)\n",
    "# normalize the data\n",
    "data_with_channel_norm = data_with_channel / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7JGqjxc3ZEo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_with_channel_norm, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Here is a summary of our dataset : \\n')\n",
    "\n",
    "print('the shape of the TRAIN labels : ',y_train.shape)\n",
    "\n",
    "print('the shape of the TRAIN data (EXTRA DIM FOR CNN) : ',X_train.shape)\n",
    "print('the shape of the TEST data : ',X_test.shape)\n",
    "\n",
    "print('\\nWe have more than 200 test samples :: GOOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jgz5dLO4Dnm"
   },
   "outputs": [],
   "source": [
    "plot_first_two_frames(X_train[0]) # Sample test\n",
    "print('the label is : ',y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvGtQ-6fKFok"
   },
   "outputs": [],
   "source": [
    "X_train[0,0,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLT7tXoD4oRD"
   },
   "source": [
    "###**Part 2 model creation and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e87d2yuo4kNc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "input_shape = (2, 120, 160, 1)\n",
    "num_classes = 3  # Number of classes to predict\n",
    "dropout_ratio = 0.5  # Dropout rate\n",
    "\n",
    "max_epochs = 30  # maxmimum number of epochs to be iterated\n",
    "batch_size = 8   # batch size for the training data set\n",
    "batch_shuffle = True   # shuffle the training data prior to batching before each epoch\n",
    "num_hidden_nodes = 32 # number of nodes in hidden fully connected layer\n",
    "\n",
    "loss = 'categorical_crossentropy'  # loss (cost) function to be minimised by the optimiser\n",
    "metrics = ['categorical_accuracy']  # network accuracy metric to be determined after each epoch\n",
    "\n",
    "optimizer_type = Adam(learning_rate=0.0001)  # optimisation algorithm: SGD stochastic gradient decent\n",
    "validtrain_split_ratio = 0.2  # % of the seen dataset to be put aside for validation, rest is for training\n",
    "\n",
    "\n",
    "# MODEL STARTING POINT\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional and pooling layers\n",
    "down_01 = TimeDistributed(Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same'))(inputs)\n",
    "down_01 = TimeDistributed(Activation('relu'))(down_01)\n",
    "down_01_pool = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(down_01)\n",
    "\n",
    "down_02 = TimeDistributed(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same'))(down_01_pool)\n",
    "down_02 = TimeDistributed(Activation('relu'))(down_02)\n",
    "down_02_pool = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(down_02)\n",
    "\n",
    "down_03 = TimeDistributed(Conv2D(filters=12, kernel_size=(3, 3), strides=(1, 1), padding='same'))(down_02_pool)\n",
    "down_03 = TimeDistributed(Activation('relu'))(down_03)\n",
    "down_03_pool = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(down_03)\n",
    "\n",
    "# Flatten and dense layers\n",
    "flatten = TimeDistributed(Flatten())(down_03_pool)\n",
    "dense_01 = TimeDistributed(Dropout(dropout_ratio))(flatten)\n",
    "dense_01 = TimeDistributed(Dense(num_hidden_nodes))(dense_01)\n",
    "dense_01 = TimeDistributed(Activation('sigmoid'))(dense_01)\n",
    "\n",
    "# Concatenation of features from the last time-distributed dense layer\n",
    "concat = Concatenate(axis=1)([dense_01[:, 0, :], dense_01[:, 1, :]])\n",
    "\n",
    "# Final dense layers for classification\n",
    "dense_02 = Dense(num_classes)(concat)\n",
    "outputs = Activation('softmax')(dense_02)\n",
    "# MODEL ENDING POINT\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1IsQjzoAxwM"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)\n",
    "# display a summary of the compiled neural network\n",
    "print(model.summary())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u2pvttQ7Zvc"
   },
   "outputs": [],
   "source": [
    "print('* Training the compiled network *')\n",
    "print()\n",
    "\n",
    "history = model.fit(X_train, y_train, \\\n",
    "                    batch_size=batch_size, \\\n",
    "                    epochs=max_epochs, \\\n",
    "                    validation_split=validtrain_split_ratio, \\\n",
    "                    shuffle=batch_shuffle)\n",
    "\n",
    "print()\n",
    "print('Training completed')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4VpTLaor_SF"
   },
   "source": [
    "### **Part 3 : optimize the model to get better performences**\n",
    "\n",
    "Their is multiple ways to optimize a CNN-RNN. The ones we are going to focus on are the followings :\n",
    "\n",
    "\n",
    "\n",
    "*   Use more than 2 frames for one sample to capture more informations\n",
    "*   Adding neurons to the network\n",
    "*   Concatenate with the two previous outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DTuggYYtQDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFiVlGR6ckIP7eDEhVsrGp",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
